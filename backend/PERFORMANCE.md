# Оптимизация производительности

## Проблема: Медленные ответы локальных моделей

Локальные языковые модели (Ollama, LocalAI) работают медленнее облачных API, так как:
- Обработка происходит на вашем компьютере
- Зависит от мощности CPU/GPU
- Модель llama2 (7B параметров) требует времени на генерацию

## Решения для ускорения

### 1. Уменьшить количество токенов

В файле `.env`:
```env
MAX_TOKENS=300  # Вместо 1000 (более короткие ответы, но быстрее)
```

### 2. Использовать более легкую модель

Скачайте более быструю модель:
```bash
# Более легкие модели (быстрее, но менее точные)
ollama pull phi          # 2.7B параметров
ollama pull gemma:2b     # 2B параметров
ollama pull tinyllama    # 1.1B параметров

# Или более мощные, но все еще быстрые
ollama pull mistral      # 7B, но оптимизированная
ollama pull neural-chat  # 7B, оптимизированная для чата
```

Затем в `.env`:
```env
OLLAMA_MODEL=phi
# или
OLLAMA_MODEL=gemma:2b
```

### 3. Использовать GPU (если доступен)

Ollama автоматически использует GPU, если он доступен. Проверьте:
```bash
ollama show llama2
```

Если GPU не используется, установите CUDA версию Ollama.

### 4. Оптимизировать параметры генерации

В `.env`:
```env
TEMPERATURE=0.5  # Меньше креативности = быстрее
MAX_TOKENS=300   # Меньше токенов = быстрее
```

### 5. Использовать streaming (в разработке)

Streaming позволит видеть ответ по мере генерации, что создаст ощущение быстрого ответа.

## Текущие настройки производительности

- **MAX_TOKENS**: 500 для Ollama (по умолчанию)
- **Используется chat API**: Более эффективный формат для Ollama
- **Оптимизированный формат сообщений**: Прямая передача messages без конвертации

## Рекомендации

### Для быстрых ответов:
```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=phi
MAX_TOKENS=200
TEMPERATURE=0.5
```

### Для баланса скорости и качества:
```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral
MAX_TOKENS=400
TEMPERATURE=0.7
```

### Для максимального качества (медленнее):
```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama2
MAX_TOKENS=1000
TEMPERATURE=0.7
```

## Ожидаемое время ответа

- **phi/gemma:2b**: 2-5 секунд
- **mistral/neural-chat**: 5-15 секунд
- **llama2**: 10-30 секунд (зависит от CPU)

Для ускорения используйте более легкие модели или уменьшите MAX_TOKENS.

